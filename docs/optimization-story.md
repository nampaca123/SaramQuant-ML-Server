# Python 대규모 연산 서비스 최적화 실전기

## 배경

3,300개 이상의 주식 종목에 대해 1년치 일봉 데이터를 수집하고, 종목마다 23개의 기술적 지표를 계산하는 파이프라인을 만들었다. 최초 실행 시 약 62만 건의 가격 데이터를 외부 API에서 가져와 DB에 저장하고, 이를 기반으로 2,553개 종목의 지표를 계산해야 한다.

이 과정에서 "코드가 돌아가기는 하는데 너무 느리다"는 전형적인 문제를 만났고, 병목을 분석해서 약 **9배** 속도 향상을 달성했다.

---

## 1. 병목은 코드가 아니라 I/O에 있었다

### 최적화 전 상황

초기 버전은 기능적으로 완전히 올바르게 동작했다. 하지만 KOSPI 하나를 수집하는 데 **60분 이상**이 걸렸다. 처음에는 "Python이 느려서 그런가" 또는 "API 호출이 많아서 어쩔 수 없다"고 생각할 수 있다. 하지만 실제 측정을 해보면 이야기가 달라진다.

하루 처리에 약 10초가 걸렸는데, 이걸 분해하면:

| 작업 | 소요 시간 | 비율 |
|------|-----------|------|
| KRX API 호출 + 응답 대기 | ~1.5초 | 15% |
| API 쓰로틀(rate limit 방지) | 1.0초 | 10% |
| **DB INSERT (종목별 개별 호출)** | **~7.5초** | **75%** |

CPU 연산이 아니라, **DB와의 네트워크 왕복(round-trip)**이 전체 시간의 75%를 차지했다.

### 왜 DB가 병목이 되었나

원래 코드의 구조를 개념적으로 설명하면 이렇다:

```
하루치 데이터를 API에서 받음 (KOSPI 전 종목, ~900개 행)
    ↓
for 종목 in 900개:
    DB에 INSERT 1건 실행   ← 매번 네트워크 왕복 발생
```

DB가 로컬에 있으면 이 패턴도 빠르다. 하지만 이 서비스는 클라우드 DB(Supabase, AWS 서울 리전)를 사용한다. INSERT 한 건마다:

1. 클라이언트 → 서버로 SQL 전송 (네트워크)
2. 서버에서 파싱 + 인덱스 업데이트
3. 결과를 클라이언트로 반환 (네트워크)

한 건당 대략 8ms라고 해도, 900건이면 7.2초다. 이것이 하루 10초 중 7.5초를 설명한다.

### 해결: Bulk INSERT

수정된 구조:

```
하루치 데이터를 API에서 받음 (KOSPI 전 종목, ~900개 행)
    ↓
900건을 메모리에 리스트로 모음
    ↓
DB에 INSERT 1건 실행 (900개 행을 한 번에)   ← 네트워크 왕복 1회
```

SQL로 표현하면, "하나씩 보내기"에서:

```sql
INSERT INTO daily_prices VALUES (1, '2025-01-02', 50000, ...);
INSERT INTO daily_prices VALUES (2, '2025-01-02', 30000, ...);
-- ... 900번 반복
```

"한 번에 보내기"로 바꾼 것이다:

```sql
INSERT INTO daily_prices VALUES
  (1, '2025-01-02', 50000, ...),
  (2, '2025-01-02', 30000, ...),
  -- ... 900개 행
;
```

네트워크 왕복이 900회 → 1회로 줄어든다. 결과는 하루 처리 시간이 **10초 → 1.1초**로 감소. 약 9배 빨라졌다.

### 교훈

> **"빠른 작업을 만 번 하는 것"과 "같은 양의 작업을 한 번에 하는 것"은 완전히 다르다.**
>
> 특히 네트워크가 끼어 있으면, 작업 자체보다 "보내고 받는 오버헤드"가 지배적이 된다. 이걸 batch 처리라고 부른다.

---

## 2. N+1 쿼리 문제

### 발견

수집 전에 "DB에 있는 최신 날짜"를 조회해서, 그 다음 날부터만 수집하는 증분 수집 로직이 있었다. 그런데 이 "최신 날짜 찾기"가 이렇게 구현되어 있었다:

```
for 종목 in 1,648개 KOSPI 종목:
    SELECT MAX(date) FROM daily_prices WHERE stock_id = {종목ID}
```

1,648번의 개별 쿼리를 날린다. 이건 ORM이나 API 서버에서 흔히 발생하는 **N+1 문제**의 전형이다. 해답은 JOIN을 활용한 단일 쿼리:

```sql
SELECT MAX(dp.date)
FROM daily_prices dp
JOIN stocks s ON dp.stock_id = s.id
WHERE s.market = 'KR_KOSPI' AND s.is_active = true
```

1,648번의 쿼리가 1번으로 줄어든다. JOIN은 DB 엔진 내부에서 처리되므로 네트워크 왕복 없이 해결된다.

### 교훈

> **DB에 질문하는 횟수를 최소화하라.** "이 데이터 줘, 저 데이터 줘"를 반복하지 말고, "이 조건에 맞는 데이터를 한 번에 줘"라고 요청하라. SQL의 JOIN, GROUP BY, 서브쿼리는 이 목적을 위해 존재한다.

---

## 3. 중복 방지: UPSERT 패턴

### 문제

매일 같은 파이프라인을 실행하면, 어제 이미 넣은 데이터를 오늘 다시 넣으려고 할 수 있다. 두 가지 접근이 가능하다:

**방법 A**: 넣기 전에 이미 있는지 확인
```
SELECT로 확인 → 없으면 INSERT
```
문제: 확인과 삽입 사이에 다른 프로세스가 끼어들 수 있고 (race condition), 쿼리도 2배로 늘어난다.

**방법 B**: 일단 넣되, 충돌하면 업데이트 (UPSERT)
```sql
INSERT INTO daily_prices (stock_id, date, open, high, low, close, volume)
VALUES (...)
ON CONFLICT (stock_id, date) DO UPDATE SET
    open = EXCLUDED.open,
    close = EXCLUDED.close, ...
```

이 서비스는 방법 B를 사용한다. `ON CONFLICT ... DO UPDATE`는 "이미 같은 키가 있으면 에러 대신 업데이트해라"는 의미다. 이를 통해:

1. 중복 데이터가 들어와도 에러가 나지 않음
2. 데이터가 수정된 경우 자동으로 최신 값으로 갱신
3. 존재 여부를 미리 확인하는 추가 쿼리가 불필요

### 증분 수집과의 조합

UPSERT는 "만약 중복되면" 처리하는 안전망이고, 증분 수집은 "처음부터 중복을 피하는" 전략이다. 둘을 조합하면:

```
DB에서 최신 날짜 조회 (JOIN 1회)  ← 증분 수집
    ↓
최신 날짜 + 1일부터만 API 호출    ← 불필요한 데이터 수집 방지
    ↓
UPSERT로 저장                     ← 혹시 중복되어도 안전
```

일상 운영에서는 증분 수집 덕분에 API 호출이 1~2회로 줄어들고, UPSERT는 에지 케이스(장애 후 재실행 등)를 안전하게 처리한다.

---

## 4. 벡터 연산: 왜 for문 없이 계산해야 하는가

### Python의 근본적 한계

3,300개 종목 × 23개 지표를 계산해야 한다. Python의 for문으로 하면:

```python
# 느린 방식
result = []
for i in range(len(prices)):
    result.append(prices[i] * weights[i])
```

Python은 **인터프리터 언어**다. for문의 매 반복마다 타입 체크, 메모리 관리, 바이트코드 해석이 일어난다. 100만 번 반복하면 100만 번의 오버헤드가 쌓인다.

### pandas와 numpy의 벡터 연산

같은 계산을 pandas/numpy로 하면:

```python
# 빠른 방식
result = prices * weights   # 내부적으로 C로 한 번에 처리
```

겉보기에는 한 줄이지만, 내부에서는 **C로 작성된 최적화된 루프**가 돌아간다. Python 인터프리터의 오버헤드가 사라진다. 동일한 연산이 수십~수백 배 빠르다.

### 실제 적용 예시

이 서비스의 지표 계산은 대부분 벡터 연산으로 구현되어 있다.

**이동평균(SMA)**: `close.rolling(window=20).mean()`

pandas의 rolling 함수는 C 레벨에서 슬라이딩 윈도우 합산을 수행한다. Python으로 직접 구현하면 매 윈도우마다 20개를 더하는 이중 루프가 되지만, pandas는 이전 합산값을 재활용하는 O(n) 알고리즘을 C로 구현해 놓았다.

**OBV(거래량 지표)**: `(volume * np.sign(close.diff())).cumsum()`

"종가가 올랐으면 거래량을 더하고, 내렸으면 빼는" 로직이다. for문으로 짜면 매번 if-else 분기가 필요하지만, numpy의 `sign()` 함수로 방향을 벡터화하고, `cumsum()`으로 누적합을 한 번에 계산한다.

**WMA(가중 이동평균)**: `np.convolve(values, weights, mode="valid")`

numpy의 `convolve`는 **합성곱(convolution)** 연산이다. 수학적으로는 가중 평균의 슬라이딩 윈도우를 한 번의 연산으로 처리한다. 이 함수는 내부적으로 BLAS(Basic Linear Algebra Subprograms) 라이브러리를 활용하며, CPU의 SIMD 명령어(한 번에 여러 숫자를 처리하는 하드웨어 기능)까지 사용할 수 있다.

### 벡터화가 불가능한 경우

23개 지표 중 **Parabolic SAR**만 유일하게 for문을 사용한다. 이 지표는 "이전 값의 결과에 따라 다음 계산 방향이 바뀌는" 구조라서 — 즉 이전 상태에 의존하는 **순차적(state-dependent)** 알고리즘이라서 — 본질적으로 병렬화가 불가능하다.

하지만 이것은 전체 계산의 1/23에 불과하고, 나머지 22개 지표가 벡터화되어 있기 때문에, 전체 2,553개 종목의 23개 지표 계산이 **27초**만에 완료된다.

### 교훈

> **Python으로 대규모 숫자 연산을 해야 한다면, for문 대신 pandas/numpy의 벡터 연산을 써라.** 이것은 "코딩 스타일"의 문제가 아니라 **수십~수백 배의 성능 차이**를 만드는 아키텍처 결정이다.

---

## 5. 측정하지 않으면 최적화할 수 없다

이 전체 과정에서 가장 중요한 원칙은 "추측하지 말고 측정하라"였다.

### 잘못된 추측의 예

처음 병목 분석을 했을 때, "매일 새 DB 커넥션을 생성하는 것이 문제"라는 진단이 있었다. 그럴듯하게 들렸지만, 실제로는 **커넥션 풀(Connection Pool)**을 사용하고 있었기 때문에 커넥션 생성 비용은 거의 없었다. 커넥션 풀은 미리 만들어둔 커넥션을 돌려가며 사용하는 구조로, "빌려오고 반납하는" 비용은 마이크로초 수준이다.

이 진단을 믿고 커넥션 관리를 리팩터링했다면, 복잡성만 높이고 성능은 개선되지 않았을 것이다.

### 올바른 측정 방법

실제로 효과가 있었던 것은 **로그 타임스탬프 비교**였다:

```
21:02:44 - "collecting 366 days" 시작
21:04:27 - "10/366 days done"        → 10일에 103초 = 일당 10.3초
```

이 10.3초를 "API 호출 시간 + 쓰로틀 + DB 시간"으로 분해했을 때, DB가 75%를 차지한다는 것을 확인하고 나서야 올바른 최적화 대상을 잡을 수 있었다.

### 최적화 전후 비교

| 항목 | 최적화 전 | 최적화 후 | 개선 |
|------|-----------|-----------|------|
| KOSPI 1년 수집 | ~61분 | ~6.7분 | **9.1x** |
| 전체 KR 파이프라인 | ~120분+ (추정) | ~16분 | **7.5x** |
| 일일 운영 시 전체 | ~3분 | ~1.5분 | 2x |
| DB 최신날짜 조회 | 1,648 쿼리 | 1 쿼리 | **1,648x** |

### 교훈

> **"느리다"고 느낄 때, 어디가 느린지 측정부터 하라.** CPU가 느린 건지, 네트워크가 느린 건지, 디스크가 느린 건지에 따라 해법이 완전히 달라진다. 측정 없는 최적화는 도박이다.

---

## 정리: 대규모 데이터 파이프라인의 5가지 원칙

1. **Batch 처리**: 네트워크 너머의 시스템과 대화할 때는 한 건씩이 아니라 묶어서 보내라.
2. **N+1 제거**: DB에 N번 물어볼 것을 JOIN으로 1번에 해결하라.
3. **UPSERT + 증분 수집**: 필요한 것만 가져오되, 중복은 DB 레벨에서 안전하게 처리하라.
4. **벡터 연산**: 숫자 계산은 Python for문이 아니라 pandas/numpy에게 맡겨라.
5. **측정 우선**: 추측하지 말고, 어디가 느린지 확인한 뒤 최적화하라.
